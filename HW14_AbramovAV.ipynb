{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea04edf",
   "metadata": {},
   "source": [
    "### Домашняя работа к уроку 14\n",
    "### Студент: Абрамов А.В."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff47105",
   "metadata": {},
   "source": [
    "1.  Дообучить берт на задачу NER\n",
    "2.  Дообучить GPT на генерацию текста\n",
    "3. Дообучить T5 на задачу суммаризации текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bcdd0599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:23:11.472985Z",
     "start_time": "2024-02-13T20:23:10.756642Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d6160c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:11:03.524993Z",
     "start_time": "2024-02-13T20:11:02.243421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>composition</th>\n",
       "      <th>cooking_type</th>\n",
       "      <th>Инструкции</th>\n",
       "      <th>dish_type</th>\n",
       "      <th>Дата</th>\n",
       "      <th>photo</th>\n",
       "      <th>source</th>\n",
       "      <th>composition_inter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>рассольник классический с перловкой и солеными...</td>\n",
       "      <td>[{'Перловка': 0.1, 'unit': 'стак. (200 мл)'}, ...</td>\n",
       "      <td>варка,жарка</td>\n",
       "      <td>Подготовить указанные ингредиенты для приготов...</td>\n",
       "      <td>первое</td>\n",
       "      <td>05.06.2015</td>\n",
       "      <td>photo_1000menu_1.jpg</td>\n",
       "      <td>https://1000.menu/cooking/33395-rassolnik-s-pe...</td>\n",
       "      <td>[{'product_id': 4253, 'name_source': 'Перловая...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Суп пюре из белокочаной капусты</td>\n",
       "      <td>[{'Капуста белокочанная': 50.0, 'unit': 'гр'},...</td>\n",
       "      <td>варка</td>\n",
       "      <td>Необходимые ингредиенты\\r\\nНарезаем лук, морко...</td>\n",
       "      <td>первое</td>\n",
       "      <td>27.06.2015</td>\n",
       "      <td>photo_1000menu_2.jpg</td>\n",
       "      <td>https://1000.menu/cooking/25399-sup-pure-iz-be...</td>\n",
       "      <td>[{'product_id': 2286, 'name_source': 'Капуста ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Постные щи из квашеной капусты</td>\n",
       "      <td>[{'Капуста квашеная': 116.7, 'unit': 'гр'}, {'...</td>\n",
       "      <td>варка,жарка,тушение</td>\n",
       "      <td>Честно признаюсь, у меня не было репы на момен...</td>\n",
       "      <td>первое</td>\n",
       "      <td>12.02.2013</td>\n",
       "      <td>photo_1000menu_3.jpg</td>\n",
       "      <td>https://1000.menu/cooking/5159-postnje-shchi</td>\n",
       "      <td>[{'product_id': 0, 'name_source': 'Капуста ква...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Тюря- простой суп быстро и вкусно</td>\n",
       "      <td>[{'Квас': 0.2, 'unit': 'л'}, {'Лук репчатый': ...</td>\n",
       "      <td>сырое</td>\n",
       "      <td>\\r\\nНачинаем мы приготовление тюри с того, что...</td>\n",
       "      <td>первое</td>\n",
       "      <td>02.03.2011</td>\n",
       "      <td>photo_1000menu_4.jpg</td>\n",
       "      <td>https://1000.menu/cooking/5085-turya</td>\n",
       "      <td>[{'product_id': 0, 'name_source': 'Квас', 'uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Фасолевый суп из красной фасоли</td>\n",
       "      <td>[{'Вода': 0.3, 'unit': 'л'}, {'Картошка': 0.3,...</td>\n",
       "      <td>варка</td>\n",
       "      <td>Подготовить ингредиенты. Для приготовления суп...</td>\n",
       "      <td>первое</td>\n",
       "      <td>28.01.2013</td>\n",
       "      <td>photo_1000menu_5.jpg</td>\n",
       "      <td>https://1000.menu/cooking/38765-fasolevyi-sup-...</td>\n",
       "      <td>[{'product_id': 828, 'name_source': 'Вода', 'u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               name  \\\n",
       "0           0  рассольник классический с перловкой и солеными...   \n",
       "1           1                    Суп пюре из белокочаной капусты   \n",
       "2           2                     Постные щи из квашеной капусты   \n",
       "3           3                  Тюря- простой суп быстро и вкусно   \n",
       "4           4                    Фасолевый суп из красной фасоли   \n",
       "\n",
       "                                         composition         cooking_type  \\\n",
       "0  [{'Перловка': 0.1, 'unit': 'стак. (200 мл)'}, ...          варка,жарка   \n",
       "1  [{'Капуста белокочанная': 50.0, 'unit': 'гр'},...                варка   \n",
       "2  [{'Капуста квашеная': 116.7, 'unit': 'гр'}, {'...  варка,жарка,тушение   \n",
       "3  [{'Квас': 0.2, 'unit': 'л'}, {'Лук репчатый': ...                сырое   \n",
       "4  [{'Вода': 0.3, 'unit': 'л'}, {'Картошка': 0.3,...                варка   \n",
       "\n",
       "                                          Инструкции dish_type        Дата  \\\n",
       "0  Подготовить указанные ингредиенты для приготов...    первое  05.06.2015   \n",
       "1  Необходимые ингредиенты\\r\\nНарезаем лук, морко...    первое  27.06.2015   \n",
       "2  Честно признаюсь, у меня не было репы на момен...    первое  12.02.2013   \n",
       "3  \\r\\nНачинаем мы приготовление тюри с того, что...    первое  02.03.2011   \n",
       "4  Подготовить ингредиенты. Для приготовления суп...    первое  28.01.2013   \n",
       "\n",
       "                  photo                                             source  \\\n",
       "0  photo_1000menu_1.jpg  https://1000.menu/cooking/33395-rassolnik-s-pe...   \n",
       "1  photo_1000menu_2.jpg  https://1000.menu/cooking/25399-sup-pure-iz-be...   \n",
       "2  photo_1000menu_3.jpg       https://1000.menu/cooking/5159-postnje-shchi   \n",
       "3  photo_1000menu_4.jpg               https://1000.menu/cooking/5085-turya   \n",
       "4  photo_1000menu_5.jpg  https://1000.menu/cooking/38765-fasolevyi-sup-...   \n",
       "\n",
       "                                   composition_inter  \n",
       "0  [{'product_id': 4253, 'name_source': 'Перловая...  \n",
       "1  [{'product_id': 2286, 'name_source': 'Капуста ...  \n",
       "2  [{'product_id': 0, 'name_source': 'Капуста ква...  \n",
       "3  [{'product_id': 0, 'name_source': 'Квас', 'uni...  \n",
       "4  [{'product_id': 828, 'name_source': 'Вода', 'u...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec = pd.read_csv('all_recepies_inter.csv', sep='\\t')\n",
    "df_rec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4546b0e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:11:04.005961Z",
     "start_time": "2024-02-13T20:11:03.988571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27884, 10)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "273b15df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:11:04.761547Z",
     "start_time": "2024-02-13T20:11:04.749422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовить указанные ингредиенты для приготовления рассольника с перловой крупой. Мясной бульон сварить заранее из говядины или из курицы, также можно сварить и вегетарианский суп - на воде. Обычно я рассольник варю без томатной пасты, но тут для разнообразия решила добавить - это по желанию. Из специй соль, чёрный перец горошком, душистый перец.\r\n",
      "Перловую крупу промыть до чистой воды.\r\n",
      "В горячий бульон добавить промытую перловку и варить на среднем огне.\r\n",
      "Для рассольника лучше брать кислые, очень солёные огурцы. Если же огурцы обычные, то рекомендуется в рассольник добавлять из сам рассол от огурцов. Солёные огурцы достать из рассола и натереть на крупной тёрке.\r\n",
      "Картофель помыть, обсушить, очистить. Нарезать кубиками. Пока очередь картофеля не подошла, положить его в воду.\r\n",
      "Морковь, лук, чеснок очистить. Морковь натереть на крупной тёрке, лук, сельдерей, чеснок мелко порезать.\r\n",
      "Обжарить в масле овощи, добавив томатной пасты. Томатная паста по желанию.\r\n",
      "Минут через двадцать добавить к перловой крупе нарезанный картофель и продолжать варить на среднем огне.\r\n",
      "Добавить через десять минут в суп обжаренные овощи. Посолить по вкусу.\r\n",
      "Ещё через десять минут добавить натёртый солёный огурец. Положить в суп чёрный перец и душистый горошком.\r\n",
      "Проварить рассольник с огурцами пять минут, выключить нагрев. Добавить нарезанную свежую зелень. дать постоять супу на плите минут 10 - 15 и можно подавать.\r\n",
      "При подаче в рассольник в каждую тарелку положить свежую сметану и зелень.\n"
     ]
    }
   ],
   "source": [
    "print(df_rec.loc[0, 'Инструкции'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f1acbd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:11:05.305957Z",
     "start_time": "2024-02-13T20:11:05.292992Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df_rec.loc[:5000, 'Инструкции']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fdae5241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:13:34.407760Z",
     "start_time": "2024-02-13T20:13:34.388417Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_text_files(data_json, dest_path):\n",
    "    with open(dest_path, 'w') as f:\n",
    "        data = ''\n",
    "        for texts in data_json:\n",
    "            summary = str(texts).strip()\n",
    "            summary = re.sub(r\"\\s\", \" \", summary)\n",
    "            data += summary + \"  \"\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6488f683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:13:37.542638Z",
     "start_time": "2024-02-13T20:13:37.524402Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4233282d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:13:38.221413Z",
     "start_time": "2024-02-13T20:13:38.212411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669    1. Разогреть духовку до 180 градусов.\\r\\n2. С ...\n",
       "1230    Капусту тонко нашинкуйте.\\r\\nПереложите капуст...\n",
       "2156    1. Порезать курицу на крупные куски.\\r\\n2. Пол...\n",
       "1269    Подготовьте продукты.\\r\\nОвощи и зелень мелко ...\n",
       "4489    1. Чеснок очистить и пропустить через пресс. И...\n",
       "                              ...                        \n",
       "3609    1. Ножки опалить, выскоблить, промыть нескольк...\n",
       "3961    1. Мелкую картошку очистить, нарезать крупными...\n",
       "4572    1. Грибы залить холодной водой и оставить на 8...\n",
       "736     Нарежьте репчатый лук соломкой толщиной не бол...\n",
       "3193    1. Репчатый лук и картофель почистить и мелко ...\n",
       "Name: Инструкции, Length: 4250, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2e5dcb40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:13:40.741171Z",
     "start_time": "2024-02-13T20:13:39.700688Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2248' in position 96367: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbuild_text_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_dataset.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m build_text_files(test,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_dataset.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[97], line 8\u001b[0m, in \u001b[0;36mbuild_text_files\u001b[1;34m(data_json, dest_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m     summary \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, summary)\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m summary \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1251.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u2248' in position 96367: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f67cbc1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:14:22.898451Z",
     "start_time": "2024-02-13T20:14:22.880498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 4250\n",
      "Test dataset length: 751\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset length: \"+ str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7da4bcfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:18:16.569307Z",
     "start_time": "2024-02-13T20:18:11.420131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5028e430644fca927a50dbde7998f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d47eb3beca43cca8505bae4b9c0695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a0ab9ca24a40e98f70bfd9d7753c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849388d693014a7abfcbbdb6abdb1958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e95ee74d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:19:31.320992Z",
     "start_time": "2024-02-13T20:19:30.972032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input file path test_dataset.txt not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[0;32m     13\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, test_dataset, data_collator\n\u001b[1;32m---> 17\u001b[0m train_dataset, test_dataset, data_collator \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 7\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(train_path, test_path, tokenizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(train_path, test_path, tokenizer):\n\u001b[0;32m      2\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m TextDataset(\n\u001b[0;32m      3\u001b[0m           tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m      4\u001b[0m           file_path\u001b[38;5;241m=\u001b[39mtrain_path,\n\u001b[0;32m      5\u001b[0m           block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[0;32m     13\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, test_dataset, data_collator\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:60\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[1;34m(self, tokenizer, file_path, block_size, overwrite_cache, cache_dir)\u001b[0m\n\u001b[0;32m     53\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     54\u001b[0m     DEPRECATION_WARNING\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m     ),\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput file path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m block_size \u001b[38;5;241m=\u001b[39m block_size \u001b[38;5;241m-\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mnum_special_tokens_to_add(pair\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     64\u001b[0m directory, filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(file_path)\n",
      "\u001b[1;31mValueError\u001b[0m: Input file path test_dataset.txt not found"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "65d3a8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:25:59.197113Z",
     "start_time": "2024-02-13T20:23:26.131779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69906787f024593803646af133dbbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632246678d804cb5ae97a46e8c72760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa4c2585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:30.094742Z",
     "start_time": "2024-02-13T20:26:30.076589Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-chief\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b1c28af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:31.328613Z",
     "start_time": "2024-02-13T20:26:31.236586Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_collator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m----> 4\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39m\u001b[43mdata_collator\u001b[49m,\n\u001b[0;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m      6\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_collator' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f55d1a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:50.880511Z",
     "start_time": "2024-02-13T20:26:50.859567Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "35057db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:52.888717Z",
     "start_time": "2024-02-13T20:26:52.267587Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('gpt_chf')\n",
    "model.save_pretrained('model_gpt_chf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "790dd0bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:54.499307Z",
     "start_time": "2024-02-13T20:26:52.904658Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt_chf\")\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"model_gpt_chf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c5319d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:55.614269Z",
     "start_time": "2024-02-13T20:26:55.604295Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = \"берем свежие томаты \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ac0ac137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:56.572496Z",
     "start_time": "2024-02-13T20:26:56.548561Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer(prefix, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70a51e5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:27:13.677314Z",
     "start_time": "2024-02-13T20:27:10.580538Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "берем свежие томаты \n",
      "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &\n"
     ]
    }
   ],
   "source": [
    "size = tokens['input_ids'].shape[1]\n",
    "output = model1.generate(\n",
    "    **tokens, \n",
    "    #end_token=end_token_id,\n",
    "    do_sample=False, \n",
    "    max_length=size+50, \n",
    "    repetition_penalty=5., \n",
    "    temperature=0.5,\n",
    "    num_beams=10,\n",
    ")\n",
    "\n",
    "decoded = tokenizer.decode(output[0])\n",
    "result = decoded[len(prefix):]\n",
    "print(prefix + result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f930d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
