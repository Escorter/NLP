{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea04edf",
   "metadata": {},
   "source": [
    "### –î–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞ –∫ —É—Ä–æ–∫—É 14\n",
    "### –°—Ç—É–¥–µ–Ω—Ç: –ê–±—Ä–∞–º–æ–≤ –ê.–í."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff47105",
   "metadata": {},
   "source": [
    "1.  –î–æ–æ–±—É—á–∏—Ç—å –±–µ—Ä—Ç –Ω–∞ –∑–∞–¥–∞—á—É NER\n",
    "2.  –î–æ–æ–±—É—á–∏—Ç—å GPT –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞\n",
    "3. –î–æ–æ–±—É—á–∏—Ç—å T5 –Ω–∞ –∑–∞–¥–∞—á—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bcdd0599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:23:11.472985Z",
     "start_time": "2024-02-13T20:23:10.756642Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d6160c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:11:03.524993Z",
     "start_time": "2024-02-13T20:11:02.243421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>composition</th>\n",
       "      <th>cooking_type</th>\n",
       "      <th>–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</th>\n",
       "      <th>dish_type</th>\n",
       "      <th>–î–∞—Ç–∞</th>\n",
       "      <th>photo</th>\n",
       "      <th>source</th>\n",
       "      <th>composition_inter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>—Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Å –ø–µ—Ä–ª–æ–≤–∫–æ–π –∏ —Å–æ–ª–µ–Ω—ã–º–∏...</td>\n",
       "      <td>[{'–ü–µ—Ä–ª–æ–≤–∫–∞': 0.1, 'unit': '—Å—Ç–∞–∫. (200 –º–ª)'}, ...</td>\n",
       "      <td>–≤–∞—Ä–∫–∞,–∂–∞—Ä–∫–∞</td>\n",
       "      <td>–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∏–≥–æ—Ç–æ–≤...</td>\n",
       "      <td>–ø–µ—Ä–≤–æ–µ</td>\n",
       "      <td>05.06.2015</td>\n",
       "      <td>photo_1000menu_1.jpg</td>\n",
       "      <td>https://1000.menu/cooking/33395-rassolnik-s-pe...</td>\n",
       "      <td>[{'product_id': 4253, 'name_source': '–ü–µ—Ä–ª–æ–≤–∞—è...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>–°—É–ø –ø—é—Ä–µ –∏–∑ –±–µ–ª–æ–∫–æ—á–∞–Ω–æ–π –∫–∞–ø—É—Å—Ç—ã</td>\n",
       "      <td>[{'–ö–∞–ø—É—Å—Ç–∞ –±–µ–ª–æ–∫–æ—á–∞–Ω–Ω–∞—è': 50.0, 'unit': '–≥—Ä'},...</td>\n",
       "      <td>–≤–∞—Ä–∫–∞</td>\n",
       "      <td>–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã\\r\\n–ù–∞—Ä–µ–∑–∞–µ–º –ª—É–∫, –º–æ—Ä–∫–æ...</td>\n",
       "      <td>–ø–µ—Ä–≤–æ–µ</td>\n",
       "      <td>27.06.2015</td>\n",
       "      <td>photo_1000menu_2.jpg</td>\n",
       "      <td>https://1000.menu/cooking/25399-sup-pure-iz-be...</td>\n",
       "      <td>[{'product_id': 2286, 'name_source': '–ö–∞–ø—É—Å—Ç–∞ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>–ü–æ—Å—Ç–Ω—ã–µ —â–∏ –∏–∑ –∫–≤–∞—à–µ–Ω–æ–π –∫–∞–ø—É—Å—Ç—ã</td>\n",
       "      <td>[{'–ö–∞–ø—É—Å—Ç–∞ –∫–≤–∞—à–µ–Ω–∞—è': 116.7, 'unit': '–≥—Ä'}, {'...</td>\n",
       "      <td>–≤–∞—Ä–∫–∞,–∂–∞—Ä–∫–∞,—Ç—É—à–µ–Ω–∏–µ</td>\n",
       "      <td>–ß–µ—Å—Ç–Ω–æ –ø—Ä–∏–∑–Ω–∞—é—Å—å, —É –º–µ–Ω—è –Ω–µ –±—ã–ª–æ —Ä–µ–ø—ã –Ω–∞ –º–æ–º–µ–Ω...</td>\n",
       "      <td>–ø–µ—Ä–≤–æ–µ</td>\n",
       "      <td>12.02.2013</td>\n",
       "      <td>photo_1000menu_3.jpg</td>\n",
       "      <td>https://1000.menu/cooking/5159-postnje-shchi</td>\n",
       "      <td>[{'product_id': 0, 'name_source': '–ö–∞–ø—É—Å—Ç–∞ –∫–≤–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>–¢—é—Ä—è- –ø—Ä–æ—Å—Ç–æ–π —Å—É–ø –±—ã—Å—Ç—Ä–æ –∏ –≤–∫—É—Å–Ω–æ</td>\n",
       "      <td>[{'–ö–≤–∞—Å': 0.2, 'unit': '–ª'}, {'–õ—É–∫ —Ä–µ–ø—á–∞—Ç—ã–π': ...</td>\n",
       "      <td>—Å—ã—Ä–æ–µ</td>\n",
       "      <td>\\r\\n–ù–∞—á–∏–Ω–∞–µ–º –º—ã –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ —Ç—é—Ä–∏ —Å —Ç–æ–≥–æ, —á—Ç–æ...</td>\n",
       "      <td>–ø–µ—Ä–≤–æ–µ</td>\n",
       "      <td>02.03.2011</td>\n",
       "      <td>photo_1000menu_4.jpg</td>\n",
       "      <td>https://1000.menu/cooking/5085-turya</td>\n",
       "      <td>[{'product_id': 0, 'name_source': '–ö–≤–∞—Å', 'uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>–§–∞—Å–æ–ª–µ–≤—ã–π —Å—É–ø –∏–∑ –∫—Ä–∞—Å–Ω–æ–π —Ñ–∞—Å–æ–ª–∏</td>\n",
       "      <td>[{'–í–æ–¥–∞': 0.3, 'unit': '–ª'}, {'–ö–∞—Ä—Ç–æ—à–∫–∞': 0.3,...</td>\n",
       "      <td>–≤–∞—Ä–∫–∞</td>\n",
       "      <td>–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã. –î–ª—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Å—É–ø...</td>\n",
       "      <td>–ø–µ—Ä–≤–æ–µ</td>\n",
       "      <td>28.01.2013</td>\n",
       "      <td>photo_1000menu_5.jpg</td>\n",
       "      <td>https://1000.menu/cooking/38765-fasolevyi-sup-...</td>\n",
       "      <td>[{'product_id': 828, 'name_source': '–í–æ–¥–∞', 'u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               name  \\\n",
       "0           0  —Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Å –ø–µ—Ä–ª–æ–≤–∫–æ–π –∏ —Å–æ–ª–µ–Ω—ã–º–∏...   \n",
       "1           1                    –°—É–ø –ø—é—Ä–µ –∏–∑ –±–µ–ª–æ–∫–æ—á–∞–Ω–æ–π –∫–∞–ø—É—Å—Ç—ã   \n",
       "2           2                     –ü–æ—Å—Ç–Ω—ã–µ —â–∏ –∏–∑ –∫–≤–∞—à–µ–Ω–æ–π –∫–∞–ø—É—Å—Ç—ã   \n",
       "3           3                  –¢—é—Ä—è- –ø—Ä–æ—Å—Ç–æ–π —Å—É–ø –±—ã—Å—Ç—Ä–æ –∏ –≤–∫—É—Å–Ω–æ   \n",
       "4           4                    –§–∞—Å–æ–ª–µ–≤—ã–π —Å—É–ø –∏–∑ –∫—Ä–∞—Å–Ω–æ–π —Ñ–∞—Å–æ–ª–∏   \n",
       "\n",
       "                                         composition         cooking_type  \\\n",
       "0  [{'–ü–µ—Ä–ª–æ–≤–∫–∞': 0.1, 'unit': '—Å—Ç–∞–∫. (200 –º–ª)'}, ...          –≤–∞—Ä–∫–∞,–∂–∞—Ä–∫–∞   \n",
       "1  [{'–ö–∞–ø—É—Å—Ç–∞ –±–µ–ª–æ–∫–æ—á–∞–Ω–Ω–∞—è': 50.0, 'unit': '–≥—Ä'},...                –≤–∞—Ä–∫–∞   \n",
       "2  [{'–ö–∞–ø—É—Å—Ç–∞ –∫–≤–∞—à–µ–Ω–∞—è': 116.7, 'unit': '–≥—Ä'}, {'...  –≤–∞—Ä–∫–∞,–∂–∞—Ä–∫–∞,—Ç—É—à–µ–Ω–∏–µ   \n",
       "3  [{'–ö–≤–∞—Å': 0.2, 'unit': '–ª'}, {'–õ—É–∫ —Ä–µ–ø—á–∞—Ç—ã–π': ...                —Å—ã—Ä–æ–µ   \n",
       "4  [{'–í–æ–¥–∞': 0.3, 'unit': '–ª'}, {'–ö–∞—Ä—Ç–æ—à–∫–∞': 0.3,...                –≤–∞—Ä–∫–∞   \n",
       "\n",
       "                                          –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ dish_type        –î–∞—Ç–∞  \\\n",
       "0  –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∏–≥–æ—Ç–æ–≤...    –ø–µ—Ä–≤–æ–µ  05.06.2015   \n",
       "1  –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã\\r\\n–ù–∞—Ä–µ–∑–∞–µ–º –ª—É–∫, –º–æ—Ä–∫–æ...    –ø–µ—Ä–≤–æ–µ  27.06.2015   \n",
       "2  –ß–µ—Å—Ç–Ω–æ –ø—Ä–∏–∑–Ω–∞—é—Å—å, —É –º–µ–Ω—è –Ω–µ –±—ã–ª–æ —Ä–µ–ø—ã –Ω–∞ –º–æ–º–µ–Ω...    –ø–µ—Ä–≤–æ–µ  12.02.2013   \n",
       "3  \\r\\n–ù–∞—á–∏–Ω–∞–µ–º –º—ã –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ —Ç—é—Ä–∏ —Å —Ç–æ–≥–æ, —á—Ç–æ...    –ø–µ—Ä–≤–æ–µ  02.03.2011   \n",
       "4  –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã. –î–ª—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Å—É–ø...    –ø–µ—Ä–≤–æ–µ  28.01.2013   \n",
       "\n",
       "                  photo                                             source  \\\n",
       "0  photo_1000menu_1.jpg  https://1000.menu/cooking/33395-rassolnik-s-pe...   \n",
       "1  photo_1000menu_2.jpg  https://1000.menu/cooking/25399-sup-pure-iz-be...   \n",
       "2  photo_1000menu_3.jpg       https://1000.menu/cooking/5159-postnje-shchi   \n",
       "3  photo_1000menu_4.jpg               https://1000.menu/cooking/5085-turya   \n",
       "4  photo_1000menu_5.jpg  https://1000.menu/cooking/38765-fasolevyi-sup-...   \n",
       "\n",
       "                                   composition_inter  \n",
       "0  [{'product_id': 4253, 'name_source': '–ü–µ—Ä–ª–æ–≤–∞—è...  \n",
       "1  [{'product_id': 2286, 'name_source': '–ö–∞–ø—É—Å—Ç–∞ ...  \n",
       "2  [{'product_id': 0, 'name_source': '–ö–∞–ø—É—Å—Ç–∞ –∫–≤–∞...  \n",
       "3  [{'product_id': 0, 'name_source': '–ö–≤–∞—Å', 'uni...  \n",
       "4  [{'product_id': 828, 'name_source': '–í–æ–¥–∞', 'u...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec = pd.read_csv('all_recepies_inter.csv', sep='\\t')\n",
    "df_rec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4546b0e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:11:04.005961Z",
     "start_time": "2024-02-13T20:11:03.988571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27884, 10)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "273b15df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:11:04.761547Z",
     "start_time": "2024-02-13T20:11:04.749422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —É–∫–∞–∑–∞–Ω–Ω—ã–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫–∞ —Å –ø–µ—Ä–ª–æ–≤–æ–π –∫—Ä—É–ø–æ–π. –ú—è—Å–Ω–æ–π –±—É–ª—å–æ–Ω —Å–≤–∞—Ä–∏—Ç—å –∑–∞—Ä–∞–Ω–µ–µ –∏–∑ –≥–æ–≤—è–¥–∏–Ω—ã –∏–ª–∏ –∏–∑ –∫—É—Ä–∏—Ü—ã, —Ç–∞–∫–∂–µ –º–æ–∂–Ω–æ —Å–≤–∞—Ä–∏—Ç—å –∏ –≤–µ–≥–µ—Ç–∞—Ä–∏–∞–Ω—Å–∫–∏–π —Å—É–ø - –Ω–∞ –≤–æ–¥–µ. –û–±—ã—á–Ω–æ —è —Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫ –≤–∞—Ä—é –±–µ–∑ —Ç–æ–º–∞—Ç–Ω–æ–π –ø–∞—Å—Ç—ã, –Ω–æ —Ç—É—Ç –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ—à–∏–ª–∞ –¥–æ–±–∞–≤–∏—Ç—å - —ç—Ç–æ –ø–æ –∂–µ–ª–∞–Ω–∏—é. –ò–∑ —Å–ø–µ—Ü–∏–π —Å–æ–ª—å, —á—ë—Ä–Ω—ã–π –ø–µ—Ä–µ—Ü –≥–æ—Ä–æ—à–∫–æ–º, –¥—É—à–∏—Å—Ç—ã–π –ø–µ—Ä–µ—Ü.\r\n",
      "–ü–µ—Ä–ª–æ–≤—É—é –∫—Ä—É–ø—É –ø—Ä–æ–º—ã—Ç—å –¥–æ —á–∏—Å—Ç–æ–π –≤–æ–¥—ã.\r\n",
      "–í –≥–æ—Ä—è—á–∏–π –±—É–ª—å–æ–Ω –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–º—ã—Ç—É—é –ø–µ—Ä–ª–æ–≤–∫—É –∏ –≤–∞—Ä–∏—Ç—å –Ω–∞ —Å—Ä–µ–¥–Ω–µ–º –æ–≥–Ω–µ.\r\n",
      "–î–ª—è —Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫–∞ –ª—É—á—à–µ –±—Ä–∞—Ç—å –∫–∏—Å–ª—ã–µ, –æ—á–µ–Ω—å —Å–æ–ª—ë–Ω—ã–µ –æ–≥—É—Ä—Ü—ã. –ï—Å–ª–∏ –∂–µ –æ–≥—É—Ä—Ü—ã –æ–±—ã—á–Ω—ã–µ, —Ç–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤ —Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫ –¥–æ–±–∞–≤–ª—è—Ç—å –∏–∑ —Å–∞–º —Ä–∞—Å—Å–æ–ª –æ—Ç –æ–≥—É—Ä—Ü–æ–≤. –°–æ–ª—ë–Ω—ã–µ –æ–≥—É—Ä—Ü—ã –¥–æ—Å—Ç–∞—Ç—å –∏–∑ —Ä–∞—Å—Å–æ–ª–∞ –∏ –Ω–∞—Ç–µ—Ä–µ—Ç—å –Ω–∞ –∫—Ä—É–ø–Ω–æ–π —Ç—ë—Ä–∫–µ.\r\n",
      "–ö–∞—Ä—Ç–æ—Ñ–µ–ª—å –ø–æ–º—ã—Ç—å, –æ–±—Å—É—à–∏—Ç—å, –æ—á–∏—Å—Ç–∏—Ç—å. –ù–∞—Ä–µ–∑–∞—Ç—å –∫—É–±–∏–∫–∞–º–∏. –ü–æ–∫–∞ –æ—á–µ—Ä–µ–¥—å –∫–∞—Ä—Ç–æ—Ñ–µ–ª—è –Ω–µ –ø–æ–¥–æ—à–ª–∞, –ø–æ–ª–æ–∂–∏—Ç—å –µ–≥–æ –≤ –≤–æ–¥—É.\r\n",
      "–ú–æ—Ä–∫–æ–≤—å, –ª—É–∫, —á–µ—Å–Ω–æ–∫ –æ—á–∏—Å—Ç–∏—Ç—å. –ú–æ—Ä–∫–æ–≤—å –Ω–∞—Ç–µ—Ä–µ—Ç—å –Ω–∞ –∫—Ä—É–ø–Ω–æ–π —Ç—ë—Ä–∫–µ, –ª—É–∫, —Å–µ–ª—å–¥–µ—Ä–µ–π, —á–µ—Å–Ω–æ–∫ –º–µ–ª–∫–æ –ø–æ—Ä–µ–∑–∞—Ç—å.\r\n",
      "–û–±–∂–∞—Ä–∏—Ç—å –≤ –º–∞—Å–ª–µ –æ–≤–æ—â–∏, –¥–æ–±–∞–≤–∏–≤ —Ç–æ–º–∞—Ç–Ω–æ–π –ø–∞—Å—Ç—ã. –¢–æ–º–∞—Ç–Ω–∞—è –ø–∞—Å—Ç–∞ –ø–æ –∂–µ–ª–∞–Ω–∏—é.\r\n",
      "–ú–∏–Ω—É—Ç —á–µ—Ä–µ–∑ –¥–≤–∞–¥—Ü–∞—Ç—å –¥–æ–±–∞–≤–∏—Ç—å –∫ –ø–µ—Ä–ª–æ–≤–æ–π –∫—Ä—É–ø–µ –Ω–∞—Ä–µ–∑–∞–Ω–Ω—ã–π –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å –∏ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –≤–∞—Ä–∏—Ç—å –Ω–∞ —Å—Ä–µ–¥–Ω–µ–º –æ–≥–Ω–µ.\r\n",
      "–î–æ–±–∞–≤–∏—Ç—å —á–µ—Ä–µ–∑ –¥–µ—Å—è—Ç—å –º–∏–Ω—É—Ç –≤ —Å—É–ø –æ–±–∂–∞—Ä–µ–Ω–Ω—ã–µ –æ–≤–æ—â–∏. –ü–æ—Å–æ–ª–∏—Ç—å –ø–æ –≤–∫—É—Å—É.\r\n",
      "–ï—â—ë —á–µ—Ä–µ–∑ –¥–µ—Å—è—Ç—å –º–∏–Ω—É—Ç –¥–æ–±–∞–≤–∏—Ç—å –Ω–∞—Ç—ë—Ä—Ç—ã–π —Å–æ–ª—ë–Ω—ã–π –æ–≥—É—Ä–µ—Ü. –ü–æ–ª–æ–∂–∏—Ç—å –≤ —Å—É–ø —á—ë—Ä–Ω—ã–π –ø–µ—Ä–µ—Ü –∏ –¥—É—à–∏—Å—Ç—ã–π –≥–æ—Ä–æ—à–∫–æ–º.\r\n",
      "–ü—Ä–æ–≤–∞—Ä–∏—Ç—å —Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫ —Å –æ–≥—É—Ä—Ü–∞–º–∏ –ø—è—Ç—å –º–∏–Ω—É—Ç, –≤—ã–∫–ª—é—á–∏—Ç—å –Ω–∞–≥—Ä–µ–≤. –î–æ–±–∞–≤–∏—Ç—å –Ω–∞—Ä–µ–∑–∞–Ω–Ω—É—é —Å–≤–µ–∂—É—é –∑–µ–ª–µ–Ω—å. –¥–∞—Ç—å –ø–æ—Å—Ç–æ—è—Ç—å —Å—É–ø—É –Ω–∞ –ø–ª–∏—Ç–µ –º–∏–Ω—É—Ç 10 - 15 –∏ –º–æ–∂–Ω–æ –ø–æ–¥–∞–≤–∞—Ç—å.\r\n",
      "–ü—Ä–∏ –ø–æ–¥–∞—á–µ –≤ —Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫ –≤ –∫–∞–∂–¥—É—é —Ç–∞—Ä–µ–ª–∫—É –ø–æ–ª–æ–∂–∏—Ç—å —Å–≤–µ–∂—É—é —Å–º–µ—Ç–∞–Ω—É –∏ –∑–µ–ª–µ–Ω—å.\n"
     ]
    }
   ],
   "source": [
    "print(df_rec.loc[0, '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f1acbd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:11:05.305957Z",
     "start_time": "2024-02-13T20:11:05.292992Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df_rec.loc[:5000, '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fdae5241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:13:34.407760Z",
     "start_time": "2024-02-13T20:13:34.388417Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_text_files(data_json, dest_path):\n",
    "    with open(dest_path, 'w') as f:\n",
    "        data = ''\n",
    "        for texts in data_json:\n",
    "            summary = str(texts).strip()\n",
    "            summary = re.sub(r\"\\s\", \" \", summary)\n",
    "            data += summary + \"  \"\n",
    "        f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6488f683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:13:37.542638Z",
     "start_time": "2024-02-13T20:13:37.524402Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4233282d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:13:38.221413Z",
     "start_time": "2024-02-13T20:13:38.212411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2669    1. –†–∞–∑–æ–≥—Ä–µ—Ç—å –¥—É—Ö–æ–≤–∫—É –¥–æ 180 –≥—Ä–∞–¥—É—Å–æ–≤.\\r\\n2. –° ...\n",
       "1230    –ö–∞–ø—É—Å—Ç—É —Ç–æ–Ω–∫–æ –Ω–∞—à–∏–Ω–∫—É–π—Ç–µ.\\r\\n–ü–µ—Ä–µ–ª–æ–∂–∏—Ç–µ –∫–∞–ø—É—Å—Ç...\n",
       "2156    1. –ü–æ—Ä–µ–∑–∞—Ç—å –∫—É—Ä–∏—Ü—É –Ω–∞ –∫—Ä—É–ø–Ω—ã–µ –∫—É—Å–∫–∏.\\r\\n2. –ü–æ–ª...\n",
       "1269    –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –ø—Ä–æ–¥—É–∫—Ç—ã.\\r\\n–û–≤–æ—â–∏ –∏ –∑–µ–ª–µ–Ω—å –º–µ–ª–∫–æ ...\n",
       "4489    1. –ß–µ—Å–Ω–æ–∫ –æ—á–∏—Å—Ç–∏—Ç—å –∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —á–µ—Ä–µ–∑ –ø—Ä–µ—Å—Å. –ò...\n",
       "                              ...                        \n",
       "3609    1. –ù–æ–∂–∫–∏ –æ–ø–∞–ª–∏—Ç—å, –≤—ã—Å–∫–æ–±–ª–∏—Ç—å, –ø—Ä–æ–º—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫...\n",
       "3961    1. –ú–µ–ª–∫—É—é –∫–∞—Ä—Ç–æ—à–∫—É –æ—á–∏—Å—Ç–∏—Ç—å, –Ω–∞—Ä–µ–∑–∞—Ç—å –∫—Ä—É–ø–Ω—ã–º–∏...\n",
       "4572    1. –ì—Ä–∏–±—ã –∑–∞–ª–∏—Ç—å —Ö–æ–ª–æ–¥–Ω–æ–π –≤–æ–¥–æ–π –∏ –æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞ 8...\n",
       "736     –ù–∞—Ä–µ–∂—å—Ç–µ —Ä–µ–ø—á–∞—Ç—ã–π –ª—É–∫ —Å–æ–ª–æ–º–∫–æ–π —Ç–æ–ª—â–∏–Ω–æ–π –Ω–µ –±–æ–ª...\n",
       "3193    1. –†–µ–ø—á–∞—Ç—ã–π –ª—É–∫ –∏ –∫–∞—Ä—Ç–æ—Ñ–µ–ª—å –ø–æ—á–∏—Å—Ç–∏—Ç—å –∏ –º–µ–ª–∫–æ ...\n",
       "Name: –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, Length: 4250, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2e5dcb40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:13:40.741171Z",
     "start_time": "2024-02-13T20:13:39.700688Z"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2248' in position 96367: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mbuild_text_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_dataset.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m build_text_files(test,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_dataset.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[97], line 8\u001b[0m, in \u001b[0;36mbuild_text_files\u001b[1;34m(data_json, dest_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m     summary \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, summary)\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m summary \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\cp1251.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u2248' in position 96367: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "build_text_files(train,'train_dataset.txt')\n",
    "build_text_files(test,'test_dataset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f67cbc1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:14:22.898451Z",
     "start_time": "2024-02-13T20:14:22.880498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 4250\n",
      "Test dataset length: 751\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset length: \"+ str(len(train)))\n",
    "print(\"Test dataset length: \"+ str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7da4bcfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:18:16.569307Z",
     "start_time": "2024-02-13T20:18:11.420131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed5028e430644fca927a50dbde7998f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d47eb3beca43cca8505bae4b9c0695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a0ab9ca24a40e98f70bfd9d7753c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849388d693014a7abfcbbdb6abdb1958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/574 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")\n",
    "\n",
    "train_path = 'train_dataset.txt'\n",
    "test_path = 'test_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e95ee74d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:19:31.320992Z",
     "start_time": "2024-02-13T20:19:30.972032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input file path test_dataset.txt not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[0;32m     13\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, test_dataset, data_collator\n\u001b[1;32m---> 17\u001b[0m train_dataset, test_dataset, data_collator \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[106], line 7\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(train_path, test_path, tokenizer)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(train_path, test_path, tokenizer):\n\u001b[0;32m      2\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m TextDataset(\n\u001b[0;32m      3\u001b[0m           tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m      4\u001b[0m           file_path\u001b[38;5;241m=\u001b[39mtrain_path,\n\u001b[0;32m      5\u001b[0m           block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[0;32m     13\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, test_dataset, data_collator\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:60\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[1;34m(self, tokenizer, file_path, block_size, overwrite_cache, cache_dir)\u001b[0m\n\u001b[0;32m     53\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     54\u001b[0m     DEPRECATION_WARNING\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     55\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m     ),\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput file path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m block_size \u001b[38;5;241m=\u001b[39m block_size \u001b[38;5;241m-\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mnum_special_tokens_to_add(pair\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     64\u001b[0m directory, filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(file_path)\n",
      "\u001b[1;31mValueError\u001b[0m: Input file path test_dataset.txt not found"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_path, test_path, tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset, test_dataset, data_collator\n",
    "\n",
    "train_dataset, test_dataset, data_collator = load_dataset(train_path, test_path, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "65d3a8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:25:59.197113Z",
     "start_time": "2024-02-13T20:23:26.131779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69906787f024593803646af133dbbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632246678d804cb5ae97a46e8c72760e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/551M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"sberbank-ai/rugpt3small_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fa4c2585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:30.094742Z",
     "start_time": "2024-02-13T20:26:30.076589Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-chief\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=4, # batch size for training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    eval_steps = 400, # Number of update steps between two evaluations.\n",
    "    save_steps=800, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b1c28af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:31.328613Z",
     "start_time": "2024-02-13T20:26:31.236586Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_collator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m----> 4\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39m\u001b[43mdata_collator\u001b[49m,\n\u001b[0;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m      6\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_dataset\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_collator' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f55d1a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:50.880511Z",
     "start_time": "2024-02-13T20:26:50.859567Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "35057db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:52.888717Z",
     "start_time": "2024-02-13T20:26:52.267587Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('gpt_chf')\n",
    "model.save_pretrained('model_gpt_chf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "790dd0bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:54.499307Z",
     "start_time": "2024-02-13T20:26:52.904658Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt_chf\")\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\"model_gpt_chf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c5319d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:55.614269Z",
     "start_time": "2024-02-13T20:26:55.604295Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = \"–±–µ—Ä–µ–º —Å–≤–µ–∂–∏–µ —Ç–æ–º–∞—Ç—ã \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ac0ac137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:26:56.572496Z",
     "start_time": "2024-02-13T20:26:56.548561Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer(prefix, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "70a51e5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-13T20:27:13.677314Z",
     "start_time": "2024-02-13T20:27:10.580538Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–±–µ—Ä–µ–º —Å–≤–µ–∂–∏–µ —Ç–æ–º–∞—Ç—ã \n",
      "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &\n"
     ]
    }
   ],
   "source": [
    "size = tokens['input_ids'].shape[1]\n",
    "output = model1.generate(\n",
    "    **tokens, \n",
    "    #end_token=end_token_id,\n",
    "    do_sample=False, \n",
    "    max_length=size+50, \n",
    "    repetition_penalty=5., \n",
    "    temperature=0.5,\n",
    "    num_beams=10,\n",
    ")\n",
    "\n",
    "decoded = tokenizer.decode(output[0])\n",
    "result = decoded[len(prefix):]\n",
    "print(prefix + result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f930d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
