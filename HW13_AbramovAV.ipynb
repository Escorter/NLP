{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ebbc551",
   "metadata": {},
   "source": [
    "### –î–æ–º–∞—à–Ω—è—è —Ä–∞–±–æ—Ç–∞ –∫ —É—Ä–æ–∫—É 13\n",
    "### –°—Ç—É–¥–µ–Ω—Ç: –ê–±—Ä–∞–º–æ–≤ –ê.–í."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff6d79f",
   "metadata": {},
   "source": [
    "–° –ø–æ–º–æ—â—å—é –±–µ—Ä—Ç–æ–≤ —Ä–µ—à–∏—Ç—å –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "404f212d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T19:22:53.380740Z",
     "start_time": "2024-02-09T19:22:49.559848Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5638116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T19:23:20.188380Z",
     "start_time": "2024-02-09T19:23:15.343704Z"
    }
   },
   "outputs": [],
   "source": [
    "data = datasets.load_dataset('merionum/ru_paraphraser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a541bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T19:23:22.785619Z",
     "start_time": "2024-02-09T19:23:22.774649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'id_1', 'id_2', 'text_1', 'text_2', 'class'],\n",
       "        num_rows: 7227\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'id_1', 'id_2', 'text_1', 'text_2', 'class'],\n",
       "        num_rows: 1924\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa98213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T19:23:58.205799Z",
     "start_time": "2024-02-09T19:23:52.175486Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"IlyaGusev/xlm_roberta_large_headline_cause_simple\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cfed15f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T19:24:00.000879Z",
     "start_time": "2024-02-09T19:23:59.823612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5901c2963bb44f60baf35002d901e6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_list = sorted(set(data['train']['class']))\n",
    "labels2id = { key:id for id, key in enumerate(label_list)}\n",
    "\n",
    "def tokenize_and_align_labels(tokenizer, labels2id):    \n",
    "    def tokenize_and_align_labels_(examples):\n",
    "#         tokenized_inputs = tokenizer([[text_1, text_2] for (text_1, text_2) in zip(examples['text_1'],examples['text_2'])])\n",
    "        tokenized_inputs = tokenizer(examples['text_1'],examples['text_2'], truncation=True)\n",
    "#         tokenized_inputs[\"labels\"] = [[labels2id[label] for _ in range(len(tokenized_inputs[\"input_ids\"][i]))] for i, label in enumerate(examples['class'])]\n",
    "        tokenized_inputs[\"labels\"] = [labels2id[label] for label in examples['class']]\n",
    "#         tokenized_inputs[\"labels\"] = [label for label in examples['class']]\n",
    "        return tokenized_inputs\n",
    "    return tokenize_and_align_labels_\n",
    "tokenized_datasets = data.map(tokenize_and_align_labels(tokenizer, labels2id), batched=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109a6a6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T19:24:11.163806Z",
     "start_time": "2024-02-09T19:24:09.799243Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_12356\\1814618623.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bb34857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T19:24:15.077841Z",
     "start_time": "2024-02-09T19:24:15.070860Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d2aaf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T19:24:18.135982Z",
     "start_time": "2024-02-09T19:24:18.095590Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"paraphras\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-6,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.05,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac9cb4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T22:36:40.456145Z",
     "start_time": "2024-02-09T19:24:53.430587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1356' max='1356' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1356/1356 3:11:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.715486</td>\n",
       "      <td>0.676195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.753300</td>\n",
       "      <td>0.731248</td>\n",
       "      <td>0.680353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.596600</td>\n",
       "      <td>0.730197</td>\n",
       "      <td>0.691268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1356, training_loss=0.642567288559095, metrics={'train_runtime': 11506.8519, 'train_samples_per_second': 1.884, 'train_steps_per_second': 0.118, 'total_flos': 1819440138501054.0, 'train_loss': 0.642567288559095, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b96e3440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T22:38:25.047141Z",
     "start_time": "2024-02-09T22:38:25.029185Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_similarity(text1, text2):\n",
    "    \"\"\" Predict the probability that two Russian sentences are paraphrases of each other. \"\"\"\n",
    "    with torch.inference_mode():\n",
    "        batch = tokenizer(\n",
    "            text1, text2, \n",
    "            truncation=True, max_length=model.config.max_position_embeddings, return_tensors='pt',\n",
    "        ).to(model.device)\n",
    "        proba = torch.softmax(model(**batch).logits, -1)\n",
    "    return proba[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ba7ca63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T22:41:36.781448Z",
     "start_time": "2024-02-09T22:41:36.685680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08449136465787888"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"–Ø –∏–¥—É –∫ —Ç–µ–±–µ\"\n",
    "text2 = \"–Ø –∏–¥—É –∑–∞ —Ç–æ–±–æ–π\"\n",
    "get_similarity(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fddd28ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-09T22:40:16.688071Z",
     "start_time": "2024-02-09T22:40:15.099321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>class</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–¶–µ–Ω—ã –Ω–∞ –Ω–µ—Ñ—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è</td>\n",
       "      <td>–ü–∞—Ä–ª–∞–º–µ–Ω—Ç –°–ª–æ–≤–∞–∫–∏–∏ –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏–ª –Ω–∞—Ä–æ–¥—ã –±—ã–≤—à–µ–≥–æ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"–ì–æ–≥–æ–ª—å-—Ü–µ–Ω—Ç—Ä\" –ø–æ–∫–∞–∂–µ—Ç –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—å —Å–∫–∞–Ω–¥–∞–ª—å–Ω–æ–≥...</td>\n",
       "      <td>–ö–µ—Ö–º–∞–Ω –∑–∞–ø—Ä–µ—Ç–∏–ª ¬´–ì–æ–≥–æ–ª—å-—Ü–µ–Ω—Ç—Ä—É¬ª –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤–∏–¥...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–ê–≥–µ–Ω—Ç: –†–§–° –≤–Ω–æ–≤—å –∑–∞–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞—Ä–ø–ª–∞—Ç—É –§–∞–±–∏–æ –ö–∞...</td>\n",
       "      <td>–°–ú–ò: –ê–≥–µ–Ω—Ç –§–∞–±–∏–æ –ö–∞–ø–µ–ª–ª–æ –≥—Ä–æ–∑–∏—Ç—Å—è –ø–æ–¥–∞—Ç—å –≤ —Å—É–¥...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–î–µ–Ω—å –ü–æ–±–µ–¥—ã –≤ –ú–æ—Å–∫–≤–µ –æ–±–µ—â–∞–µ—Ç –≤—ã–¥–∞—Ç—å—Å—è –æ–±–ª–∞—á–Ω—ã–º</td>\n",
       "      <td>–õ—é–±–ª—è–Ω–∞ –æ—Ç–ø—Ä–∞–∑–¥–Ω—É–µ—Ç –î–µ–Ω—å –ü–æ–±–µ–¥—ã –≤–º–µ—Å—Ç–µ —Å –ú–æ—Å–∫–≤–æ–π</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–ü–æ—Å–æ–ª –†–§ –≤ –°–®–ê: –†–æ—Å—Å–∏—è –±—É–¥–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–æ–ø—ã—Ç–∫...</td>\n",
       "      <td>–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –ª–æ—Ç–µ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>–í–µ—Ä—Ç–æ–ª–µ—Ç —Å 11 –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–∞–º–∏ –Ω–∞ –±–æ—Ä—Ç—É —É–ø–∞–ª –≤ –ü–∞–∫...</td>\n",
       "      <td>–í –ü–∞–∫–∏—Å—Ç–∞–Ω–µ —É–ø–∞–ª –≤–µ—Ä—Ç–æ–ª–µ—Ç —Å 11 –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–∞–º–∏</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>–°–∞–º–æ–ª–µ—Ç –≤–µ—Ä–Ω—É–ª—Å—è –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞ –∏–∑-–∑–∞...</td>\n",
       "      <td>–°–∞–º–æ–ª–µ—Ç –≤–µ—Ä–Ω—É–ª—Å—è –≤ –Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∏–π –∞—ç—Ä–æ–ø–æ—Ä—Ç –∏–∑-–∑...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>–í–∞—Å–∏–ª—å–µ–≤–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤–∏–Ω–æ–≤–Ω–æ–π –≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–µ –∏ ...</td>\n",
       "      <td>–í–∞—Å–∏–ª—å–µ–≤–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤–∏–Ω–æ–≤–Ω–æ–π –≤ —Ö–∏—â–µ–Ω–∏—è—Ö –∏ –æ—Ç–º—ã–≤...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>–ü—É—Ç–∏–Ω –ø–µ—Ä–µ–¥ –î–Ω–µ–º –ü–æ–±–µ–¥—ã –ø–æ–¥–ø–∏—Å–∞–ª —É–∫–∞–∑ –æ –ø—Ä–∏—Å–≤–æ...</td>\n",
       "      <td>–°–ö –†–§: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–º–∏—Ç–µ—Ç–∞...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>–°—É–¥ –æ–ø—Ä–∞–≤–¥–∞–ª –í–∞—Å–∏–ª—å–µ–≤—É –≤ —Ö–∏—â–µ–Ω–∏–∏ –∞–∫—Ü–∏–π –Ω–∞ –¥–≤–∞ ...</td>\n",
       "      <td>–°—É–¥ –æ–ø—Ä–∞–≤–¥–∞–ª –í–∞—Å–∏–ª—å–µ–≤—É –≤ —Ö–∏—â–µ–Ω–∏–∏ –∞–∫—Ü–∏–π –Ω–∞ 2 –º–ª...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>–ü—É—à–∫–æ–≤: —É –û–±–∞–º—ã –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ –¥—É—Ö–∞ –ª–∏—á–Ω–æ –ø–æ–∑–¥—Ä–∞–≤–∏...</td>\n",
       "      <td>–ü—É—à–∫–æ–≤: –û–±–∞–º–∞ –Ω–µ –Ω–∞—à–µ–ª –≤ —Å–µ–±–µ –¥—É—Ö–∞ –ª–∏—á–Ω–æ –ø–æ–∑–¥—Ä...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>–ú–ß–° –†–§: —Ç–µ–ª–∞ –ø–æ–≥–∏–±—à–∏—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–æ–≤ –¥–æ...</td>\n",
       "      <td>–¢–µ–ª–∞ –ø–æ–≥–∏–±—à–∏—Ö –≤ –ù–µ–ø–∞–ª–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–æ–≤ –¥...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>–î–µ–≤—è—Ç—å —Å–∞–º–æ–ª–µ—Ç–æ–≤ –í–í–° —Ä–∞–∑–≥–æ–Ω—è—Ç –æ–±–ª–∞–∫–∞ –Ω–∞–¥ –ú–æ—Å–∫–≤...</td>\n",
       "      <td>–û–±–ª–∞–∫–∞ –Ω–∞–¥ –ú–æ—Å–∫–≤–æ–π –≤ –î–µ–Ω—å –ü–æ–±–µ–¥—ã —Ä–∞–∑–≥–æ–Ω—è—Ç –¥–µ–≤—è...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>–¢—Ä–æ–∏—Ö –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤-—É–±–∏–π—Ü –ø–æ–π–º–∞–ª–∏ –≤ –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ</td>\n",
       "      <td>–í –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ —Ç—Ä–æ–µ –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤ –ø—Ä–∏–∑–Ω–∞–ª–∏—Å—å –≤ —Å–µ—Ä...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>–°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç —Å–æ–æ–±—â–∏–ª –æ –≤–∑–ª–æ–º–µ —Å–≤–æ–µ–≥–æ —Å...</td>\n",
       "      <td>–í –°–ö–† —Å–æ–æ–±—â–∏–ª–∏ –æ –≤–∑–ª–æ–º–µ —Ö–∞–∫–µ—Ä–∞–º–∏ —Å–∞–π—Ç–∞ –≤–µ–¥–æ–º—Å—Ç–≤–∞</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text_1  \\\n",
       "0                     –¶–µ–Ω—ã –Ω–∞ –Ω–µ—Ñ—Ç—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è   \n",
       "1   \"–ì–æ–≥–æ–ª—å-—Ü–µ–Ω—Ç—Ä\" –ø–æ–∫–∞–∂–µ—Ç –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å—å —Å–∫–∞–Ω–¥–∞–ª—å–Ω–æ–≥...   \n",
       "2   –ê–≥–µ–Ω—Ç: –†–§–° –≤–Ω–æ–≤—å –∑–∞–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞—Ä–ø–ª–∞—Ç—É –§–∞–±–∏–æ –ö–∞...   \n",
       "3      –î–µ–Ω—å –ü–æ–±–µ–¥—ã –≤ –ú–æ—Å–∫–≤–µ –æ–±–µ—â–∞–µ—Ç –≤—ã–¥–∞—Ç—å—Å—è –æ–±–ª–∞—á–Ω—ã–º   \n",
       "4   –ü–æ—Å–æ–ª –†–§ –≤ –°–®–ê: –†–æ—Å—Å–∏—è –±—É–¥–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–æ–ø—ã—Ç–∫...   \n",
       "5   –í–µ—Ä—Ç–æ–ª–µ—Ç —Å 11 –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–∞–º–∏ –Ω–∞ –±–æ—Ä—Ç—É —É–ø–∞–ª –≤ –ü–∞–∫...   \n",
       "6   –°–∞–º–æ–ª–µ—Ç –≤–µ—Ä–Ω—É–ª—Å—è –≤ –∞—ç—Ä–æ–ø–æ—Ä—Ç –ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∞ –∏–∑-–∑–∞...   \n",
       "7   –í–∞—Å–∏–ª—å–µ–≤–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤–∏–Ω–æ–≤–Ω–æ–π –≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–µ –∏ ...   \n",
       "8   –ü—É—Ç–∏–Ω –ø–µ—Ä–µ–¥ –î–Ω–µ–º –ü–æ–±–µ–¥—ã –ø–æ–¥–ø–∏—Å–∞–ª —É–∫–∞–∑ –æ –ø—Ä–∏—Å–≤–æ...   \n",
       "9   –°—É–¥ –æ–ø—Ä–∞–≤–¥–∞–ª –í–∞—Å–∏–ª—å–µ–≤—É –≤ —Ö–∏—â–µ–Ω–∏–∏ –∞–∫—Ü–∏–π –Ω–∞ –¥–≤–∞ ...   \n",
       "10  –ü—É—à–∫–æ–≤: —É –û–±–∞–º—ã –Ω–µ —Ö–≤–∞—Ç–∏–ª–æ –¥—É—Ö–∞ –ª–∏—á–Ω–æ –ø–æ–∑–¥—Ä–∞–≤–∏...   \n",
       "11  –ú–ß–° –†–§: —Ç–µ–ª–∞ –ø–æ–≥–∏–±—à–∏—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–æ–≤ –¥–æ...   \n",
       "12  –î–µ–≤—è—Ç—å —Å–∞–º–æ–ª–µ—Ç–æ–≤ –í–í–° —Ä–∞–∑–≥–æ–Ω—è—Ç –æ–±–ª–∞–∫–∞ –Ω–∞–¥ –ú–æ—Å–∫–≤...   \n",
       "13       –¢—Ä–æ–∏—Ö –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤-—É–±–∏–π—Ü –ø–æ–π–º–∞–ª–∏ –≤ –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ   \n",
       "14  –°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç —Å–æ–æ–±—â–∏–ª –æ –≤–∑–ª–æ–º–µ —Å–≤–æ–µ–≥–æ —Å...   \n",
       "\n",
       "                                               text_2 class predict  \n",
       "0   –ü–∞—Ä–ª–∞–º–µ–Ω—Ç –°–ª–æ–≤–∞–∫–∏–∏ –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏–ª –Ω–∞—Ä–æ–¥—ã –±—ã–≤—à–µ–≥–æ...    -1       1  \n",
       "1   –ö–µ—Ö–º–∞–Ω –∑–∞–ø—Ä–µ—Ç–∏–ª ¬´–ì–æ–≥–æ–ª—å-—Ü–µ–Ω—Ç—Ä—É¬ª –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤–∏–¥...    -1       1  \n",
       "2   –°–ú–ò: –ê–≥–µ–Ω—Ç –§–∞–±–∏–æ –ö–∞–ø–µ–ª–ª–æ –≥—Ä–æ–∑–∏—Ç—Å—è –ø–æ–¥–∞—Ç—å –≤ —Å—É–¥...    -1       1  \n",
       "3    –õ—é–±–ª—è–Ω–∞ –æ—Ç–ø—Ä–∞–∑–¥–Ω—É–µ—Ç –î–µ–Ω—å –ü–æ–±–µ–¥—ã –≤–º–µ—Å—Ç–µ —Å –ú–æ—Å–∫–≤–æ–π    -1       1  \n",
       "4   –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ –∑–∞—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ –ª–æ—Ç–µ...    -1       1  \n",
       "5         –í –ü–∞–∫–∏—Å—Ç–∞–Ω–µ —É–ø–∞–ª –≤–µ—Ä—Ç–æ–ª–µ—Ç —Å 11 –∏–Ω–æ—Å—Ç—Ä–∞–Ω—Ü–∞–º–∏     1       1  \n",
       "6   –°–∞–º–æ–ª–µ—Ç –≤–µ—Ä–Ω—É–ª—Å—è –≤ –Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫–∏–π –∞—ç—Ä–æ–ø–æ—Ä—Ç –∏–∑-–∑...     1       1  \n",
       "7   –í–∞—Å–∏–ª—å–µ–≤–∞ –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤–∏–Ω–æ–≤–Ω–æ–π –≤ —Ö–∏—â–µ–Ω–∏—è—Ö –∏ –æ—Ç–º—ã–≤...     0       1  \n",
       "8   –°–ö –†–§: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–º–∏—Ç–µ—Ç–∞...    -1       1  \n",
       "9   –°—É–¥ –æ–ø—Ä–∞–≤–¥–∞–ª –í–∞—Å–∏–ª—å–µ–≤—É –≤ —Ö–∏—â–µ–Ω–∏–∏ –∞–∫—Ü–∏–π –Ω–∞ 2 –º–ª...     1       1  \n",
       "10  –ü—É—à–∫–æ–≤: –û–±–∞–º–∞ –Ω–µ –Ω–∞—à–µ–ª –≤ —Å–µ–±–µ –¥—É—Ö–∞ –ª–∏—á–Ω–æ –ø–æ–∑–¥—Ä...     1       1  \n",
       "11  –¢–µ–ª–∞ –ø–æ–≥–∏–±—à–∏—Ö –≤ –ù–µ–ø–∞–ª–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –¥–∏–ø–ª–æ–º–∞—Ç–æ–≤ –¥...     1       1  \n",
       "12  –û–±–ª–∞–∫–∞ –Ω–∞–¥ –ú–æ—Å–∫–≤–æ–π –≤ –î–µ–Ω—å –ü–æ–±–µ–¥—ã —Ä–∞–∑–≥–æ–Ω—è—Ç –¥–µ–≤—è...     1       1  \n",
       "13  –í –ü–æ–¥–º–æ—Å–∫–æ–≤—å–µ —Ç—Ä–æ–µ –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤ –ø—Ä–∏–∑–Ω–∞–ª–∏—Å—å –≤ —Å–µ—Ä...     0       1  \n",
       "14   –í –°–ö–† —Å–æ–æ–±—â–∏–ª–∏ –æ –≤–∑–ª–æ–º–µ —Ö–∞–∫–µ—Ä–∞–º–∏ —Å–∞–π—Ç–∞ –≤–µ–¥–æ–º—Å—Ç–≤–∞     1       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2labels = { id:key for id, key in enumerate(label_list)}\n",
    "example = tokenized_datasets[\"test\"][0:15]\n",
    "tokens = tokenizer(example['text_1'], example['text_1'], padding=True, truncation=True, return_tensors='pt')\n",
    "tokens = tokens\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "\n",
    "predicted = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "classes = [id2labels[id_label] for id_label in predicted]\n",
    "df_example =pd.DataFrame({'text_1':example['text_1'], 'text_2':example['text_2'], 'class':example['class'], 'predict':classes})\n",
    "df_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d8fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
